{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ErsXOU8vfQb"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"\"\"\"\"\"\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os, json, time\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "df = pd.read_csv('filtered.csv')\n",
        "df = df.sample(n=1, random_state=None)\n",
        "df\n",
        "\n",
        "\n",
        "\n",
        "import os, json\n",
        "from openai import OpenAI\n",
        "\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "SYSTEM_INSTRUCTIONS = (\n",
        "    \"Rewrite Instagram Reel captions about college admissions into a short, fact-based dialogue between Stewie and Peter Griffin.\\n\"\n",
        "    \"Format:\\n\"\n",
        "    \"- Stewie always opens with a sharp question.\\n\"\n",
        "    \"- The content should be in assumption that the audience is a high school student who is going through the college admissions process and knows a good amount about the process.\"\n",
        "    \"- Peter provides the majority of the dialogue (≈70–80%), doing detailed fact-based explanations.\\n\"\n",
        "    \"- Stewie only asks quick questions or gives short affirmations occasionally, without cutting off Peter.\\n\"\n",
        "    \"- Alternate strictly: Stewie, Peter, Stewie, Peter… until ending.\\n\"\n",
        "    \"- Keep it to ~1 minute total of talking. That is around 130 words.\\n\"\n",
        "    \"- End with a short, clear call-to-action (CTA).\\n\\n\"\n",
        "    \"Requirements:\\n\"\n",
        "    \"- Always include concrete facts when possible: Ivy League admit rates, FAFSA deadlines, SAT/ACT ranges, AP/IB credit, internships, extracurriculars, cost of attendance.\\n\"\n",
        "    \"- Be clear, accurate, and informative. No vague advice or motivational fluff.\\n\"\n",
        "    \"- Humor should fit the characters, but facts must stay correct.\\n\"\n",
        "    \"- Should be formated Stewie: ..., Peter: ...\"\n",
        ")\n",
        "\n",
        "USER_TEMPLATE = \"\"\"Turn this Instagram Reel caption into a Stewie–Peter fact-based dialogue.\n",
        "\n",
        "INPUT:\n",
        "{raw}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def make_dialogue(transcript: str) -> str:\n",
        "    \"\"\"Returns a dense, fact-forward dialogue between Peter & Stewie.\"\"\"\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_INSTRUCTIONS},\n",
        "            {\"role\": \"user\", \"content\": USER_TEMPLATE.format(raw=transcript.strip())}\n",
        "        ],\n",
        "        temperature=0.6,\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "def generate_transcript(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df[\"dialogue\"] = df[\"caption\"].progress_apply(make_dialogue)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "final_script_df = generate_transcript(df)\n",
        "transcript = final_script_df['dialogue'].item()\n",
        "print(transcript)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests pydub\n",
        "\n",
        "import os, io, json, tempfile, requests, pathlib\n",
        "from typing import List, Tuple, Dict\n",
        "from pydub import AudioSegment\n",
        "from google.colab import files  # (left as-is; now unused)\n",
        "\n",
        "ELEVEN_API_KEY = \"\"\n",
        "ELEVEN_STEWIE_VOICE_ID = \"\"\n",
        "ELEVEN_PETER_VOICE_ID  = \"\"\n",
        "\n",
        "# Tunables\n",
        "MODEL_ID = \"eleven_multilingual_v2\"\n",
        "LINE_GAP_MS = 120  # gap inserted between lines in merged track\n",
        "VOICE_SETTINGS = {\"stability\": 0.5, \"similarity_boost\": 0.75, \"style\": 0.0, \"use_speaker_boost\": True}\n",
        "\n",
        "def _split_dialogue(dialogue_text: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"Return ordered list of (speaker, text) for lines that start with 'Stewie:' or 'Peter:'.\"\"\"\n",
        "    ordered = []\n",
        "    for raw in dialogue_text.splitlines():\n",
        "        line = raw.strip()\n",
        "        if not line or line.startswith(\"[\"):  # skip [HOOK]/[CTA]\n",
        "            continue\n",
        "        lower = line.lower()\n",
        "        if lower.startswith(\"stewie:\"):\n",
        "            ordered.append((\"Stewie\", line.split(\":\", 1)[1].strip()))\n",
        "        elif lower.startswith(\"peter:\"):\n",
        "            ordered.append((\"Peter\", line.split(\":\", 1)[1].strip()))\n",
        "    if not ordered:\n",
        "        raise ValueError(\"No dialogue lines detected (expected lines starting with 'Stewie:' or 'Peter:').\")\n",
        "    return ordered\n",
        "\n",
        "def _tts_elevenlabs(text: str, voice_id: str) -> AudioSegment:\n",
        "    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream\"\n",
        "    headers = {\"xi-api-key\": ELEVEN_API_KEY, \"accept\": \"audio/mpeg\", \"content-type\": \"application/json\"}\n",
        "    payload = {\"text\": text, \"model_id\": MODEL_ID, \"voice_settings\": VOICE_SETTINGS, \"optimize_streaming_latency\": 0}\n",
        "    r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=60)\n",
        "    r.raise_for_status()\n",
        "    return AudioSegment.from_file(io.BytesIO(r.content), format=\"mp3\")\n",
        "\n",
        "def _format_ts(ms: int) -> str:\n",
        "    h = ms // 3_600_000; ms %= 3_600_000\n",
        "    m = ms // 60_000; ms %= 60_000\n",
        "    s = ms // 1000; ms %= 1000\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n",
        "\n",
        "def _write_srt(cues: List[Tuple[int, int, str]]) -> str:\n",
        "    \"\"\"cues: list of (start_ms, end_ms, text)\"\"\"\n",
        "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".srt\"); tmp.close()\n",
        "    with open(tmp.name, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, (start, end, text) in enumerate(cues, start=1):\n",
        "            f.write(f\"{i}\\n{_format_ts(start)} --> {_format_ts(end)}\\n{text}\\n\\n\")\n",
        "    return tmp.name\n",
        "\n",
        "def generate_voiceovers_and_srt_elevenlabs(dialogue_text: str, merge: bool = True) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Returns paths:\n",
        "      - 'merged_path' (mp3): merged alternating track (if merge=True)\n",
        "      - 'stewie_path' (mp3): concatenated Stewie-only track\n",
        "      - 'peter_path'  (mp3): concatenated Peter-only track\n",
        "      - 'srt_path'    (srt): subtitles aligned to merged track (if merge=True)\n",
        "    \"\"\"\n",
        "    ordered = _split_dialogue(dialogue_text)\n",
        "\n",
        "    # Per-line synthesis for precise timing\n",
        "    per_line_audio: List[AudioSegment] = []\n",
        "    per_line_texts: List[str] = []\n",
        "    for speaker, text in ordered:\n",
        "        voice_id = ELEVEN_STEWIE_VOICE_ID if speaker == \"Stewie\" else ELEVEN_PETER_VOICE_ID\n",
        "        seg = _tts_elevenlabs(text, voice_id)\n",
        "        per_line_audio.append(seg)\n",
        "        per_line_texts.append(f\"{speaker}: {text}\")\n",
        "\n",
        "    # Build speaker-specific concatenations\n",
        "    stewie_concat = AudioSegment.silent(duration=0)\n",
        "    peter_concat  = AudioSegment.silent(duration=0)\n",
        "    for (speaker, _), seg in zip(ordered, per_line_audio):\n",
        "        if speaker == \"Stewie\":\n",
        "            stewie_concat += seg + AudioSegment.silent(duration=LINE_GAP_MS)\n",
        "        else:\n",
        "            peter_concat  += seg + AudioSegment.silent(duration=LINE_GAP_MS)\n",
        "\n",
        "    # Save individual tracks\n",
        "    stewie_fd = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\"); stewie_path = stewie_fd.name; stewie_fd.close()\n",
        "    peter_fd  = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\");  peter_path  = peter_fd.name;  peter_fd.close()\n",
        "    stewie_concat.export(stewie_path, format=\"mp3\")\n",
        "    peter_concat.export(peter_path,  format=\"mp3\")\n",
        "\n",
        "    out = {\"stewie_path\": stewie_path, \"peter_path\": peter_path}\n",
        "\n",
        "    if not merge:\n",
        "        return out\n",
        "\n",
        "    # Merge in *spoken order* and build SRT cues from actual durations\n",
        "    merged = AudioSegment.silent(duration=0)\n",
        "    cues = []\n",
        "    cursor = 0\n",
        "    for seg, text in zip(per_line_audio, per_line_texts):\n",
        "        start = cursor\n",
        "        end = start + len(seg)\n",
        "        cues.append((start, end, text))\n",
        "        merged += seg\n",
        "        cursor = end\n",
        "        # gap between lines (not captioned)\n",
        "        merged += AudioSegment.silent(duration=LINE_GAP_MS)\n",
        "        cursor += LINE_GAP_MS\n",
        "\n",
        "    merged_fd = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\"); merged_path = merged_fd.name; merged_fd.close()\n",
        "    merged.export(merged_path, format=\"mp3\")\n",
        "    srt_path = _write_srt(cues)\n",
        "\n",
        "    out.update({\"merged_path\": merged_path, \"srt_path\": srt_path})\n",
        "    return out\n",
        "\n",
        "\n",
        "dialogue = transcript\n",
        "\n",
        "out = generate_voiceovers_and_srt_elevenlabs(dialogue)\n",
        "\n",
        "# === Changed: keep files on Colab VM instead of downloading to local ===\n",
        "import shutil\n",
        "merged_dest = \"/content/merged.mp3\"\n",
        "srt_dest = \"/content/subtitles.srt\"\n",
        "shutil.copy(out[\"merged_path\"], merged_dest)\n",
        "shutil.copy(out[\"srt_path\"], srt_dest)\n",
        "print(f\"Saved merged audio to: {merged_dest}\")\n",
        "print(f\"Saved subtitles to:    {srt_dest}\")"
      ],
      "metadata": {
        "id": "-SRwwn4vwCzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -y install ffmpeg\n",
        "!pip -q install moviepy pydub\n",
        "\n",
        "\n",
        "import random\n",
        "from pathlib import Path\n",
        "from moviepy.editor import VideoFileClip\n",
        "from pydub import AudioSegment\n",
        "from IPython.display import Video, display\n",
        "\n",
        "def extract_random_clip(input_path: str,\n",
        "                        audio_path: str,\n",
        "                        output_path: str = \"output.mp4\",\n",
        "                        seed: int | None = None) -> tuple[str, float, float]:\n",
        "    \"\"\"\n",
        "    Take a random chunk of the video whose length equals the duration of audio_path (mp3).\n",
        "    Guarantees the chunk ends before the video ends.\n",
        "\n",
        "    Returns: (output_path, start_time_sec, actual_duration_sec)\n",
        "    \"\"\"\n",
        "\n",
        "    input_path = str(input_path)\n",
        "    output_path = str(output_path)\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    # Get audio duration\n",
        "    audio = AudioSegment.from_file(audio_path)\n",
        "    duration = audio.duration_seconds\n",
        "\n",
        "    # Load video metadata and clip\n",
        "    with VideoFileClip(input_path) as vid:\n",
        "        total = float(vid.duration or 0.0)\n",
        "        if total <= 0:\n",
        "            raise ValueError(\"Could not read video duration (is the file valid?)\")\n",
        "\n",
        "        # Clamp if audio is longer than video\n",
        "        dur = min(duration, total)\n",
        "\n",
        "        latest_start = max(0.0, total - dur)\n",
        "        start = 0.0 if latest_start == 0 else random.uniform(0.0, latest_start)\n",
        "        end = start + dur\n",
        "\n",
        "        sub = vid.subclip(start, end)\n",
        "        sub.write_videofile(\n",
        "            output_path,\n",
        "            codec=\"libx264\",\n",
        "            audio_codec=\"aac\",\n",
        "            temp_audiofile=\"__temp_aac.m4a\",\n",
        "            remove_temp=True,\n",
        "            threads=0,\n",
        "            preset=\"medium\",\n",
        "            fps=vid.fps or 24\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        from google.colab import files  # type: ignore\n",
        "        if Path(output_path).exists():\n",
        "            files.download(output_path)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        display(Video(output_path, embed=True))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return output_path, start, dur\n",
        "\n",
        "# Example usage: duration will be read from merged.mp3\n",
        "out, start, dur = extract_random_clip(\"minecraft.mp4\", \"merged.mp3\", output_path=\"minecraft_clip.mp4\")\n",
        "print(f\"Saved to: {out} | start={start:.3f}s | duration={dur:.3f}s\")"
      ],
      "metadata": {
        "id": "3EBygRefxsfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n",
        "\n",
        "import os\n",
        "import mimetypes\n",
        "import json\n",
        "import uuid\n",
        "import boto3\n",
        "import requests\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "\n",
        "R2_ACCOUNT_ID = \"\"\n",
        "R2_ACCESS_KEY = \"\"\n",
        "R2_SECRET_KEY = \"\"\n",
        "R2_BUCKET = \"\"\n",
        "\n",
        "CREATOMATE_API_KEY = \"\"\n",
        "CREATOMATE_TEMPLATE_ID = \"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------- R2 client (S3-compatible) ----------\n",
        "r2 = boto3.client(\n",
        "    \"s3\",\n",
        "    aws_access_key_id=R2_ACCESS_KEY,\n",
        "    aws_secret_access_key=R2_SECRET_KEY,\n",
        "    endpoint_url=f\"https://{R2_ACCOUNT_ID}.r2.cloudflarestorage.com\",\n",
        "    region_name=\"auto\",\n",
        ")\n",
        "\n",
        "def _guess_content_type(path: str, default: str = \"application/octet-stream\") -> str:\n",
        "    ctype, _ = mimetypes.guess_type(path)\n",
        "    return ctype or default\n",
        "\n",
        "def r2_upload_and_presign(local_path: str, key_prefix: str = \"inputs/\", expires_seconds: int = 3600) -> str:\n",
        "    \"\"\"Uploads a local file to R2 (private) and returns a presigned GET URL.\n",
        "       NOTE: Presigned URLs work on the R2 API hostname, not your custom domain.\n",
        "    \"\"\"\n",
        "    key = f\"{key_prefix}{uuid.uuid4().hex}_{os.path.basename(local_path)}\"\n",
        "    content_type = _guess_content_type(local_path)\n",
        "\n",
        "    # Upload (private by default)\n",
        "    r2.upload_file(\n",
        "        Filename=local_path,\n",
        "        Bucket=R2_BUCKET,\n",
        "        Key=key,\n",
        "        ExtraArgs={\"ContentType\": content_type},\n",
        "    )\n",
        "\n",
        "    # Presign GET\n",
        "    url = r2.generate_presigned_url(\n",
        "        ClientMethod=\"get_object\",\n",
        "        Params={\"Bucket\": R2_BUCKET, \"Key\": key},\n",
        "        ExpiresIn=expires_seconds,\n",
        "    )\n",
        "    return url\n",
        "\n",
        "def render_video(mp4_path: str, mp3_path: str) -> dict:\n",
        "    \"\"\"Uploads assets to R2, presigns them, and triggers a Creatomate render.\"\"\"\n",
        "    # 1) Upload + presign (ensure expiry comfortably exceeds render time)\n",
        "    mp4_url = r2_upload_and_presign(mp4_path, key_prefix=\"inputs/video/\", expires_seconds=2 * 3600)\n",
        "    mp3_url = r2_upload_and_presign(mp3_path, key_prefix=\"inputs/audio/\", expires_seconds=2 * 3600)\n",
        "\n",
        "    # 2) Creatomate render\n",
        "    url = \"https://api.creatomate.com/v2/renders\"\n",
        "    payload = {\n",
        "        \"template_id\": CREATOMATE_TEMPLATE_ID,\n",
        "        \"modifications\": {\n",
        "            \"Video.source\": mp4_url,\n",
        "            \"Audio.source\": mp3_url\n",
        "        }\n",
        "    }\n",
        "    resp = requests.post(\n",
        "        url,\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {CREATOMATE_API_KEY}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        },\n",
        "        data=json.dumps(payload),\n",
        "        timeout=60,\n",
        "    )\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()\n",
        "\n",
        "\n",
        "result = render_video(\"minecraft_clip.mp4\", \"merged.mp3\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "03kJMjOi2Pb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQu4rz7O67aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After waiting for video render"
      ],
      "metadata": {
        "id": "FZ5enmY13hi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ One-cell Colab script (Stewie higher/right/bigger; fast fly; no mirroring)\n",
        "# - Upload in Colab: input.mp4, captions.srt, stewie.png, peter.png\n",
        "# - Stewie: bottom-left-ish, flies in from LEFT and out to LEFT.\n",
        "# - Peter:  bottom-right, flies in from RIGHT and out to RIGHT.\n",
        "\n",
        "# 0) Install deps\n",
        "!pip -q install moviepy==1.0.3\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "from IPython.display import Video, display\n",
        "from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip\n",
        "\n",
        "# 2) Config\n",
        "VIDEO_PATH    = \"createomate.mp4\"\n",
        "SRT_PATH      = \"subtitles.srt\"\n",
        "STEWIE_IMG    = \"stewie.png\"\n",
        "PETER_IMG     = \"peter.png\"\n",
        "\n",
        "MARGIN_PX     = 10          # closer to the edges\n",
        "STEWIE_H_FRAC = 0.32        # Stewie a tiny bit bigger\n",
        "PETER_H_FRAC  = 0.44        # Peter size relative to video height\n",
        "FLY_DUR       = 0.06        # faster fly-in and fly-out\n",
        "OVERSHOOT_PX  = 24          # larger offscreen buffer for crisp exits\n",
        "\n",
        "# Stewie custom offsets\n",
        "STEWIE_EXTRA_RIGHT = 10     # px farther right\n",
        "STEWIE_EXTRA_UP    = 0     # px higher\n",
        "\n",
        "# 3) Parse SRT for speaker intervals\n",
        "time_pat = re.compile(\n",
        "    r\"(\\d{2}):(\\d{2}):(\\d{2}),(\\d{3})\\s*-->\\s*(\\d{2}):(\\d{2}):(\\d{2}),(\\d{3})\"\n",
        ")\n",
        "def _sec(h,m,s,ms): return int(h)*3600 + int(m)*60 + int(s) + int(ms)/1000.0\n",
        "\n",
        "def parse_srt_for_speakers(srt_path):\n",
        "    with open(srt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        blob = f.read()\n",
        "    blocks = re.split(r\"\\n\\s*\\n\", blob.strip())\n",
        "    out = []\n",
        "    for b in blocks:\n",
        "        lines = [ln for ln in b.splitlines() if ln.strip()]\n",
        "        if not lines: continue\n",
        "        mt = time_pat.search(lines[1]) if len(lines) > 1 else None\n",
        "        text_idx = 2 if mt else 1\n",
        "        if not mt:\n",
        "            mt = time_pat.search(lines[0])\n",
        "        if not mt: continue\n",
        "        sh,sm,ss,sms, eh,em,es,ems = mt.groups()\n",
        "        start, end = _sec(sh,sm,ss,sms), _sec(eh,em,es,ems)\n",
        "        if end <= start: continue\n",
        "        text = \"\\n\".join(lines[text_idx:]).strip()\n",
        "        speaker = None\n",
        "        if re.match(r\"^\\s*stewie\\s*:\", text, flags=re.I): speaker = \"stewie\"\n",
        "        elif re.match(r\"^\\s*peter\\s*:\", text, flags=re.I): speaker = \"peter\"\n",
        "        if speaker: out.append({\"start\": start, \"end\": end, \"speaker\": speaker})\n",
        "    return sorted(out, key=lambda e: e[\"start\"])\n",
        "\n",
        "# 4) Build composite with fly animations\n",
        "assert Path(VIDEO_PATH).exists(), \"createomate.mp4 not found\"\n",
        "assert Path(SRT_PATH).exists(), \"subtitles.srt not found\"\n",
        "assert Path(STEWIE_IMG).exists(), \"stewie.png not found\"\n",
        "assert Path(PETER_IMG).exists(), \"peter.png not found\"\n",
        "\n",
        "vid = VideoFileClip(VIDEO_PATH)\n",
        "vw, vh = vid.w, vid.h\n",
        "\n",
        "# Prepare base sprites\n",
        "stewie_base = ImageClip(STEWIE_IMG).resize(height=vh * STEWIE_H_FRAC)\n",
        "peter_base  = ImageClip(PETER_IMG).resize(height=vh * PETER_H_FRAC)\n",
        "\n",
        "# Resting positions\n",
        "stewie_y = vh - stewie_base.h - MARGIN_PX\n",
        "peter_y  = vh - peter_base.h  - MARGIN_PX + 10\n",
        "stewie_on_x = MARGIN_PX + STEWIE_EXTRA_RIGHT\n",
        "peter_on_x  = vw - peter_base.w - MARGIN_PX\n",
        "\n",
        "# Offscreen positions\n",
        "stewie_off_x = -stewie_base.w - OVERSHOOT_PX\n",
        "peter_off_x  = vw + OVERSHOOT_PX\n",
        "\n",
        "def smoothstep(u): return max(0.0, min(1.0, u*u*(3 - 2*u)))\n",
        "def lerp(a, b, u): return a + (b - a) * u\n",
        "\n",
        "def make_fly_clip(sprite, start, end, side):\n",
        "    dur = max(0.001, end - start)\n",
        "    t_in  = min(FLY_DUR, dur/2.0)\n",
        "    t_out = min(FLY_DUR, dur/2.0)\n",
        "    hold_start = t_in\n",
        "    hold_end   = max(hold_start, dur - t_out)\n",
        "\n",
        "    if side == \"left\":\n",
        "        x_off, x_on, y = stewie_off_x, stewie_on_x, stewie_y\n",
        "    else:\n",
        "        x_off, x_on, y = peter_off_x,  peter_on_x,  peter_y\n",
        "\n",
        "    def pos_func(t):\n",
        "        if t <= hold_start:\n",
        "            u = smoothstep(t / t_in) if t_in > 0 else 1.0\n",
        "            x = lerp(x_off, x_on, u)\n",
        "        elif t < hold_end:\n",
        "            x = x_on\n",
        "        else:\n",
        "            u = smoothstep((t - hold_end) / max(1e-6, (dur - hold_end)))\n",
        "            x = lerp(x_on, x_off, u)\n",
        "        return (x, y)\n",
        "\n",
        "    return sprite.set_start(start).set_duration(dur).set_pos(pos_func)\n",
        "\n",
        "events = parse_srt_for_speakers(SRT_PATH)\n",
        "\n",
        "clips = [vid]\n",
        "for ev in events:\n",
        "    if ev[\"speaker\"] == \"stewie\":\n",
        "        clips.append(make_fly_clip(stewie_base, ev[\"start\"], ev[\"end\"], side=\"left\"))\n",
        "    elif ev[\"speaker\"] == \"peter\":\n",
        "        clips.append(make_fly_clip(peter_base,  ev[\"start\"], ev[\"end\"], side=\"right\"))\n",
        "\n",
        "final = CompositeVideoClip(clips, size=vid.size)\n",
        "\n",
        "# 5) Render and preview\n",
        "out_name = \"output_with_characters.mp4\"\n",
        "final.write_videofile(\n",
        "    out_name,\n",
        "    codec=\"libx264\",\n",
        "    audio_codec=\"aac\",\n",
        "    fps=vid.fps or 30,\n",
        "    preset=\"medium\",\n",
        "    threads=4\n",
        ")\n",
        "\n",
        "display(Video(out_name, embed=True, html_attributes=\"controls loop muted playsinline\"))\n",
        "\n",
        "try:\n",
        "    from google.colab import files  # type: ignore\n",
        "    files.download(out_name)\n",
        "except Exception:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "yBXgPUfM3mHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install moviepy==1.0.3 srt==3.5.3 pillow==10.4.0 icrawler pandas==2.2.2 openai==1.40.6\n",
        "\n",
        "import os, re, io, random, string, tempfile\n",
        "import pandas as pd\n",
        "import srt\n",
        "from pathlib import Path\n",
        "from datetime import timedelta\n",
        "from PIL import Image\n",
        "from icrawler.builtin import GoogleImageCrawler\n",
        "from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip, vfx\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "API_KEY = \"\"\"\"\"\"\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "VIDEO_PATH = \"output_with_characters.mp4\"\n",
        "SRT_PATH   = \"subtitles.srt\"\n",
        "OUT_DIR = Path(\"downloads\")\n",
        "MANIFEST_CSV = \"overlay_manifest.csv\"\n",
        "OUT_VIDEO = \"final_output.mp4\"\n",
        "\n",
        "assert os.path.exists(VIDEO_PATH), f\"Missing {VIDEO_PATH}\"\n",
        "assert os.path.exists(SRT_PATH),   f\"Missing {SRT_PATH}\"\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def read_srt(path: str):\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return list(srt.parse(f.read()))\n",
        "\n",
        "def to_seconds(td: timedelta) -> float:\n",
        "    return td.total_seconds()\n",
        "\n",
        "def sanitize_seed(text: str) -> str:\n",
        "    t = (text or \"\").strip()\n",
        "    t = re.sub(r\"^[A-Za-z .'\\-]{2,24}:\\s+\", \"\", t)\n",
        "    t = re.sub(r\"[\\(\\[][^)\\]]*[\\)\\]]\", \"\", t)\n",
        "    t = re.sub(r\"(https?://\\S+)|(@\\w+)|(#\\w+)\", \"\", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    words = t.split()\n",
        "    return \" \".join(words[:12])\n",
        "\n",
        "def slugify(s: str, maxlen: int = 60) -> str:\n",
        "    s = re.sub(r\"[^A-Za-z0-9._\\- ]+\", \"\", s).strip().lower()\n",
        "    s = re.sub(r\"\\s+\", \"-\", s)\n",
        "    if len(s) > maxlen:\n",
        "        s = s[:maxlen].rstrip(\"-\")\n",
        "    return s or \"q\"\n",
        "\n",
        "# -----------------------------\n",
        "# OpenAI (optional) — broad college visuals (icons, campuses, forms, essays, flowcharts)\n",
        "# -----------------------------\n",
        "def refine_query(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Turn a subtitle into a <= 8-word, college-admissions-focused image query.\n",
        "    Accept logos, icons, campus/app screenshots, rankings/scholarships, SAT/ACT/GPA visuals,\n",
        "    essay prompts/supplementals/example essays, and flowcharts/diagrams/guides (College Essay Guy style).\n",
        "    \"\"\"\n",
        "    base = sanitize_seed(text)\n",
        "    if not base:\n",
        "        base = \"college admissions\"\n",
        "    if not API_KEY:\n",
        "        return f\"{base} college essay flowchart\"\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "        client = OpenAI(api_key=API_KEY)\n",
        "        system = (\n",
        "            \"Convert the caption into a concise (<= 8 words) image-search query \"\n",
        "            \"for COLLEGE ADMISSIONS visuals. Accept: logos/icons (Common App, FAFSA, Ivy League, Stanford, MIT), \"\n",
        "            \"campus photos, application screenshots, clubs/competitions, US News rankings, scholarships, SAT/ACT/GPA visuals, \"\n",
        "            \"essay prompts, supplementals, example essays, and flowcharts/diagrams/guides (College Essay Guy style). \"\n",
        "            \"Avoid vague phrasing. Output ONLY the query.\"\n",
        "        )\n",
        "        user = f\"Caption: {base}\\nReturn a short, college admissions–related image search query.\"\n",
        "        resp = client.chat.completions.create(\n",
        "            model=OPENAI_MODEL,\n",
        "            messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
        "            temperature=0.2,\n",
        "            max_tokens=24,\n",
        "        )\n",
        "        q = (resp.choices[0].message.content or \"\").strip()\n",
        "        q = re.sub(r\"[\\n\\r]+\", \" \", q).strip()\n",
        "        return q or f\"{base} college essay flowchart\"\n",
        "    except Exception:\n",
        "        return f\"{base} college essay flowchart\"\n",
        "\n",
        "# -----------------------------\n",
        "# Image crawling (broad to include diagrams/flowcharts/docs)\n",
        "# -----------------------------\n",
        "def crawl_one_image(query: str, dest_dir: Path, max_num: int = 1):\n",
        "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "    existing = [p for p in dest_dir.glob(\"*\") if p.is_file()]\n",
        "    if existing:\n",
        "        return [str(p) for p in existing]\n",
        "    crawler = GoogleImageCrawler(storage={'root_dir': str(dest_dir)})\n",
        "    try:\n",
        "        crawler.crawl(keyword=query, max_num=max_num, filters=None)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Crawl failed for '{query}': {e}\")\n",
        "        return []\n",
        "    files = [str(p) for p in dest_dir.glob(\"*\") if p.is_file()]\n",
        "    return files\n",
        "\n",
        "def add_soft_border(img: Image.Image, pad: int = 10, alpha_bg: int = 220) -> Image.Image:\n",
        "    if img.mode != \"RGBA\":\n",
        "        img = img.convert(\"RGBA\")\n",
        "    w, h = img.size\n",
        "    bg = Image.new(\"RGBA\", (w + 2*pad, h + 2*pad), (255, 255, 255, alpha_bg))\n",
        "    bg.paste(img, (pad, pad), img)\n",
        "    return bg\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Build refined queries\n",
        "# -----------------------------\n",
        "subs = read_srt(SRT_PATH)\n",
        "rows = []\n",
        "unique_queries = {}\n",
        "\n",
        "for idx, sub in enumerate(subs, start=1):\n",
        "    start_s = to_seconds(sub.start)\n",
        "    end_s   = to_seconds(sub.end)\n",
        "    if end_s <= start_s:\n",
        "        continue\n",
        "    refined = refine_query(sub.content).strip()\n",
        "    q_key = refined.lower()\n",
        "    if q_key not in unique_queries:\n",
        "        unique_queries[q_key] = refined\n",
        "    rows.append({\n",
        "        \"line_index\": idx,\n",
        "        \"start_time\": start_s,\n",
        "        \"end_time\": end_s,\n",
        "        \"text\": sub.content.strip(),\n",
        "        \"query\": refined,\n",
        "        \"image_path\": \"\"\n",
        "    })\n",
        "\n",
        "print(f\"Subtitle lines parsed: {len(rows)} | Unique refined queries: {len(unique_queries)}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Crawl images for all unique queries FIRST\n",
        "# -----------------------------\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "query_to_imagepath = {}\n",
        "\n",
        "for q_key, q in unique_queries.items():\n",
        "    folder = OUT_DIR / slugify(q)\n",
        "    files = crawl_one_image(q, folder, max_num=1)\n",
        "    if files:\n",
        "        query_to_imagepath[q] = files[0]\n",
        "        print(f\"[OK] {q} -> {files[0]}\")\n",
        "    else:\n",
        "        query_to_imagepath[q] = \"\"\n",
        "        print(f\"[MISS] {q} -> (no image)\")\n",
        "\n",
        "# Map back to rows + write manifest\n",
        "for r in rows:\n",
        "    r[\"image_path\"] = query_to_imagepath.get(r[\"query\"], \"\")\n",
        "pd.DataFrame(rows).to_csv(MANIFEST_CSV, index=False, encoding=\"utf-8\")\n",
        "print(f\"Manifest: {Path(MANIFEST_CSV).resolve()}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Build overlay clips: 85% width, top-centered, never below 40% height\n",
        "# -----------------------------\n",
        "video = VideoFileClip(VIDEO_PATH)\n",
        "video_w, video_h = video.w, video.h\n",
        "\n",
        "TARGET_W_FRAC = 0.85                   # <- 85% of video width\n",
        "MIN_IMG_W = max(140, int(video_w * 0.10))\n",
        "MAX_H = int(video_h * 0.40)            # <- images must not extend below 40% of frame height\n",
        "\n",
        "# Transitions (fast)\n",
        "FADE = 0.05\n",
        "POP  = 0.06\n",
        "SCALE_MIN = 0.60\n",
        "\n",
        "# Top-centered baseline\n",
        "X_ANCHOR = \"center\"\n",
        "TOP_MARGIN = int(video_h * 0.08)       # try to keep near the top\n",
        "\n",
        "overlay_clips = []\n",
        "\n",
        "def prepare_png_for_overlay(img_path: str):\n",
        "    \"\"\"\n",
        "    Resize to 85% width, crop so total (with border) <= MAX_H,\n",
        "    add border, return (temp_png_path, (w, h)) of final image.\n",
        "    \"\"\"\n",
        "    im = Image.open(img_path).convert(\"RGBA\")\n",
        "\n",
        "    # 1) Resize by width target (85% of video width)\n",
        "    target_w = max(MIN_IMG_W, int(video_w * TARGET_W_FRAC))\n",
        "    scale = target_w / im.width\n",
        "    new_w, new_h = target_w, int(im.height * scale)\n",
        "    im = im.resize((new_w, new_h), Image.LANCZOS)\n",
        "\n",
        "    # 2) If too tall, center-crop vertically to fit within MAX_H - pad allowance\n",
        "    pad_each = 10\n",
        "    allowable_h_before_pad = MAX_H - 2*pad_each\n",
        "    if im.height > allowable_h_before_pad:\n",
        "        crop_h = max(10, allowable_h_before_pad)\n",
        "        top = max(0, (im.height - crop_h)//2)\n",
        "        im = im.crop((0, top, im.width, top + crop_h))\n",
        "\n",
        "    # 3) Add soft border\n",
        "    im = add_soft_border(im, pad=pad_each, alpha_bg=220)\n",
        "\n",
        "    # 4) Enforce MAX_H again (in case pad pushed it over)\n",
        "    if im.height > MAX_H:\n",
        "        top = max(0, (im.height - MAX_H)//2)\n",
        "        im = im.crop((0, top, im.width, top + MAX_H))\n",
        "\n",
        "    tmp = os.path.join(tempfile.gettempdir(), \"ovl_\" + \"\".join(random.choices(string.ascii_lowercase+string.digits, k=8)) + \".png\")\n",
        "    im.save(tmp, \"PNG\")\n",
        "    return tmp, im.size  # (w, h)\n",
        "\n",
        "for r in rows:\n",
        "    img_path = r[\"image_path\"]\n",
        "    if not img_path or not os.path.exists(img_path):\n",
        "        continue\n",
        "\n",
        "    start_t = r[\"start_time\"]\n",
        "    end_t   = r[\"end_time\"]\n",
        "    dur = end_t - start_t\n",
        "    if dur <= 0.03:\n",
        "        continue\n",
        "\n",
        "    png_path, (ow, oh) = prepare_png_for_overlay(img_path)\n",
        "\n",
        "    # Position: centered horizontally, and as high as possible WITHOUT crossing 40% height.\n",
        "    # Ensure top_y + overlay_height <= MAX_H (i.e., bottom edge no lower than 40% of frame).\n",
        "    allowed_top = max(0, MAX_H - oh)     # highest y so that bottom == MAX_H\n",
        "    pos_y = min(TOP_MARGIN, allowed_top) # keep toward top but honor the 40% ceiling\n",
        "    pos = (X_ANCHOR, pos_y)\n",
        "\n",
        "    clip = (ImageClip(png_path)\n",
        "            .set_start(start_t)\n",
        "            .set_duration(dur)\n",
        "            .set_position(pos)\n",
        "            .set_opacity(1.0))\n",
        "\n",
        "    # Ultra-fast pop from center + quick fade\n",
        "    D = clip.duration\n",
        "    def scale_fun(t, D=D, P=POP, s0=SCALE_MIN):\n",
        "        if t < P:\n",
        "            return s0 + (1.0 - s0) * (t / P)\n",
        "        elif t > D - P:\n",
        "            return s0 + (1.0 - s0) * ((D - t) / P)\n",
        "        else:\n",
        "            return 1.0\n",
        "\n",
        "    clip = (clip\n",
        "            .fx(vfx.resize, scale_fun)\n",
        "            .fx(vfx.fadein, FADE)\n",
        "            .fx(vfx.fadeout, FADE))\n",
        "\n",
        "    overlay_clips.append(clip)\n",
        "\n",
        "print(f\"Overlay clips built: {len(overlay_clips)}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Render final video\n",
        "# -----------------------------\n",
        "final = CompositeVideoClip([video] + overlay_clips) if overlay_clips else video\n",
        "fps = getattr(video, \"fps\", 24) or 24\n",
        "\n",
        "final.write_videofile(\n",
        "    OUT_VIDEO,\n",
        "    codec=\"libx264\",\n",
        "    audio_codec=\"aac\",\n",
        "    fps=fps,\n",
        "    threads=4,\n",
        "    preset=\"medium\",\n",
        "    logger=None\n",
        ")\n",
        "\n",
        "print(f\"Done. Wrote: {OUT_VIDEO}\")\n",
        "try:\n",
        "    from IPython.display import Video, display\n",
        "    display(Video(OUT_VIDEO, embed=True))\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "id": "Z2htaD3w4QUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}